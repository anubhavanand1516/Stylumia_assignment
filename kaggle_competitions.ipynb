{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdKW4UoV5+wiSXDnlrpWo+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anubhavanand1516/Stylumia_assignment/blob/main/kaggle_competitions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEM9dyXgcL2N",
        "outputId": "6764e8a7-106a-48cc-a334-fd1e317f8409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: anubhavanand1516\n",
            "Your Kaggle Key: ··········\n",
            "Downloading extracting-attributes-from-fashion-images-jan-2024.zip to ./extracting-attributes-from-fashion-images-jan-2024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 508M/508M [00:17<00:00, 30.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting archive ./extracting-attributes-from-fashion-images-jan-2024/extracting-attributes-from-fashion-images-jan-2024.zip to ./extracting-attributes-from-fashion-images-jan-2024\n"
          ]
        }
      ],
      "source": [
        "!pip install -q opendatasets\n",
        "\n",
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "\n",
        "od.download('https://www.kaggle.com/competitions/extracting-attributes-from-fashion-images-jan-2024/data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "TYfELxIJcMp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_df = pd.read_csv('/content/extracting-attributes-from-fashion-images-jan-2024/train.csv')"
      ],
      "metadata": {
        "id": "HEKJ0dsyh7Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir = '/content/extracting-attributes-from-fashion-images-jan-2024/train'\n",
        "test_data_dir = '/content/extracting-attributes-from-fashion-images-jan-2024/test'\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        ")\n"
      ],
      "metadata": {
        "id": "jRt6w-y_h8aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_df['label'] = labels_df['label'].astype(str)\n",
        "batch_size = 64\n",
        "img_height = 112\n",
        "img_width = 112\n"
      ],
      "metadata": {
        "id": "60DB0SuAiJwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=labels_df,\n",
        "    directory=train_data_dir,\n",
        "    x_col='file_name',  # Column containing the image filenames\n",
        "    y_col='label',  # Column containing the labels\n",
        "    color_mode = \"rgb\",\n",
        "    subset='training',\n",
        "    batch_size=batch_size,\n",
        "    target_size=(img_height, img_width),\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESiB6QugiNrc",
        "outputId": "51918ecd-c5ad-4a22-a1a3-a903c664b838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14713 validated image filenames belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=labels_df,\n",
        "    directory=train_data_dir,\n",
        "    x_col='file_name',\n",
        "    y_col='label',\n",
        "    subset='validation',\n",
        "    batch_size=batch_size,\n",
        "    target_size=(img_height, img_width),\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuGKer5_iQoL",
        "outputId": "8bd4e028-ea53-48df-9fa0-776a44031671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3678 validated image filenames belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(7, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "nSG_KRkHiYcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-btWgfkjicZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "history = model.fit(train_generator, epochs=epochs, validation_data=validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuSclp4tifEN",
        "outputId": "3eb31e9e-5f2a-49f1-d7ea-392487edba4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "230/230 [==============================] - 1167s 5s/step - loss: 1.6540 - accuracy: 0.3596 - val_loss: 1.8052 - val_accuracy: 0.3165\n",
            "Epoch 2/20\n",
            "230/230 [==============================] - 1093s 5s/step - loss: 1.3548 - accuracy: 0.4767 - val_loss: 1.6538 - val_accuracy: 0.3464\n",
            "Epoch 3/20\n",
            "230/230 [==============================] - 1085s 5s/step - loss: 1.2070 - accuracy: 0.5372 - val_loss: 1.5772 - val_accuracy: 0.3475\n",
            "Epoch 4/20\n",
            "230/230 [==============================] - 1092s 5s/step - loss: 1.1155 - accuracy: 0.5731 - val_loss: 1.5578 - val_accuracy: 0.3864\n",
            "Epoch 5/20\n",
            "230/230 [==============================] - 1081s 5s/step - loss: 1.0390 - accuracy: 0.5960 - val_loss: 1.5341 - val_accuracy: 0.3883\n",
            "Epoch 6/20\n",
            "230/230 [==============================] - 1088s 5s/step - loss: 0.9673 - accuracy: 0.6243 - val_loss: 1.5793 - val_accuracy: 0.3662\n",
            "Epoch 7/20\n",
            "230/230 [==============================] - 1084s 5s/step - loss: 0.9024 - accuracy: 0.6422 - val_loss: 1.7295 - val_accuracy: 0.3880\n",
            "Epoch 8/20\n",
            "230/230 [==============================] - 1082s 5s/step - loss: 0.8426 - accuracy: 0.6653 - val_loss: 1.6325 - val_accuracy: 0.3888\n",
            "Epoch 9/20\n",
            "230/230 [==============================] - 1076s 5s/step - loss: 0.7814 - accuracy: 0.6937 - val_loss: 1.8209 - val_accuracy: 0.3940\n",
            "Epoch 10/20\n",
            "230/230 [==============================] - 1078s 5s/step - loss: 0.7220 - accuracy: 0.7107 - val_loss: 1.9002 - val_accuracy: 0.3834\n",
            "Epoch 11/20\n",
            "230/230 [==============================] - 1135s 5s/step - loss: 0.6763 - accuracy: 0.7334 - val_loss: 1.8869 - val_accuracy: 0.3823\n",
            "Epoch 12/20\n",
            "230/230 [==============================] - 1131s 5s/step - loss: 0.6289 - accuracy: 0.7504 - val_loss: 1.8505 - val_accuracy: 0.4043\n",
            "Epoch 13/20\n",
            "230/230 [==============================] - 1070s 5s/step - loss: 0.6005 - accuracy: 0.7600 - val_loss: 2.0643 - val_accuracy: 0.3997\n",
            "Epoch 14/20\n",
            "230/230 [==============================] - 1072s 5s/step - loss: 0.5451 - accuracy: 0.7787 - val_loss: 2.0735 - val_accuracy: 0.4038\n",
            "Epoch 15/20\n",
            "230/230 [==============================] - 1070s 5s/step - loss: 0.5096 - accuracy: 0.7911 - val_loss: 2.3225 - val_accuracy: 0.4065\n",
            "Epoch 16/20\n",
            "230/230 [==============================] - 1068s 5s/step - loss: 0.4871 - accuracy: 0.8036 - val_loss: 2.4981 - val_accuracy: 0.3899\n",
            "Epoch 17/20\n",
            "230/230 [==============================] - 1073s 5s/step - loss: 0.4653 - accuracy: 0.8132 - val_loss: 2.4260 - val_accuracy: 0.3825\n",
            "Epoch 18/20\n",
            "230/230 [==============================] - 1068s 5s/step - loss: 0.4415 - accuracy: 0.8215 - val_loss: 2.5753 - val_accuracy: 0.3948\n",
            "Epoch 19/20\n",
            "230/230 [==============================] - 1068s 5s/step - loss: 0.4062 - accuracy: 0.8330 - val_loss: 2.9144 - val_accuracy: 0.3777\n",
            "Epoch 20/20\n",
            "230/230 [==============================] - 1067s 5s/step - loss: 0.4021 - accuracy: 0.8361 - val_loss: 2.8405 - val_accuracy: 0.3983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('trouser_fit_type_model.h5')"
      ],
      "metadata": {
        "id": "FukYYwZuij0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv('/content/extracting-attributes-from-fashion-images-jan-2024/sample_submission.csv')\n"
      ],
      "metadata": {
        "id": "J1zfXm2NqKAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "X8bF2Qp5qqbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    sample_submission,\n",
        "    directory='/content/extracting-attributes-from-fashion-images-jan-2024/test',\n",
        "    x_col='file_name',\n",
        "    y_col=None,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEVVUaDqrOlP",
        "outputId": "f1f0652a-32a7-4f96-9254-71148dba2793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5751 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = model.predict(test_generator)\n",
        "predicted_labels = test_predictions.argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL2tfoOYrUE_",
        "outputId": "24cb4446-dbfc-4c64-8bc2-36955d5f72ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90/90 [==============================] - 135s 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_filenames = test_generator.filenames"
      ],
      "metadata": {
        "id": "vx27pmfpsCdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_data = {'file_name': test_filenames, 'label': predicted_labels}\n",
        "submission_df = pd.DataFrame(submission_data)\n"
      ],
      "metadata": {
        "id": "IoC6Cg6csL9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "8clYbIxNsjeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_dpJg75smrJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}